% Official ACL style integration
\documentclass[11pt]{article}
% acl package sets the required margins, fonts, and two-column layout.
% For anonymous submission add [review]; for final camera ready add \aclfinalcopy.
\usepackage{acl} % Use [review] option for anonymous submission; 'final' is default.
% NOTE: This acl.sty version does NOT define \aclfinalcopy (legacy macro). Remove it to avoid an
% "Undefined control sequence" error. For review mode use: \usepackage[review]{acl}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
% Ensure scalable Latin Modern fonts to avoid microtype expansion errors
\usepackage{lmodern}
% Disable font expansion (was causing: "expansion is only possible with scalable fonts")
\usepackage[expansion=false]{microtype}
% (times and latexsym are loaded by the ACL style; removed to avoid duplication)


% Additional packages
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{subcaption}
\usepackage{placeins}
\usepackage{tabularx}
\usepackage{array}
\usepackage{multirow}
\usepackage{url}
% (hyperref is already configured by acl.sty; removed duplicate)


% --- Code Listing Style ---
\lstset{
    basicstyle=\ttfamily\footnotesize,
    backgroundcolor=\color{gray!10},
    frame=single,
    breaklines=true,
    showstringspaces=false,
    tabsize=2
}

% --- Title and Authors ---
\title{Local Hybrid Retrieval-Augmented Document QA}

\author{Paolo Astrino \\
  Università Ca' Foscari Venezia, Italy}

\begin{document}
\maketitle


\begin{abstract}
Organizations handling sensitive documents face a critical dilemma: adopt cloud-based AI systems that offer powerful question-answering capabilities but compromise data privacy, or maintain local processing that ensures security but delivers poor accuracy. We present a question-answering system that resolves this trade-off by combining semantic understanding with keyword precision, operating entirely on local infrastructure without internet access. Our approach demonstrates that organizations can achieve competitive accuracy on complex queries across legal, scientific, and conversational documents while keeping all data on their machines. By balancing two complementary retrieval strategies and using consumer-grade hardware acceleration, the system delivers reliable answers with minimal errors, letting banks, hospitals, and law firms adopt conversational document AI without transmitting proprietary information to external providers. This work establishes that privacy and performance need not be mutually exclusive in enterprise AI deployment.

\end{abstract}

\section{Introduction}

The exponential growth of digital information has created unprecedented challenges for organizations seeking to efficiently access and utilize knowledge stored in heterogeneous document formats. Traditional keyword-based search methods fail to address complex queries or synthesize information across multiple documents \cite{lewis2020rag}. Moreover, state-of-the-art AI language models require uploading sensitive data to external cloud servers, creating significant barriers for regulated industries and organizations handling proprietary information \cite{edpb,privacyinternational}.

Retrieval-Augmented Generation (RAG) addresses these challenges by combining the generative capabilities of large language models (LLMs) with external knowledge retrieval systems \cite{arxiv2410,arxiv24102}. However, existing RAG implementations often rely on cloud-based processing or single retrieval strategies that limit their effectiveness in enterprise environments \cite{hybridretrieval}. Even systems that claim "local" operation frequently depend on external API calls for embedding generation or LLM inference, compromising data privacy.

This paper presents a fully local RAG system that operates entirely on owned infrastructure without internet access. Document processing, embedding generation using BGE (HuggingFace), hybrid retrieval combining BM25 and dense vectors, and answer synthesis via Ollama and Llama 3.2 all execute on-premises. The hybrid strategy was tuned across 10 weight configurations to identify the optimal 30\% sparse / 70\% dense balance. GPU acceleration yields 4.2× faster embedding and 3× faster inference on consumer hardware. Hallucination is quantified using LLM-as-Judge on over 1,500 query-answer pairs, and multi-dimensional metrics—covering retrieval coverage, ranking quality, extractive fidelity, and distributional statistics—are reported on commodity hardware. This work demonstrates that high-accuracy document question-answering can be achieved without sacrificing data sovereignty, offering a practical solution for healthcare, finance, and legal sectors.
\section{Related Work}

\subsection{Retrieval-Augmented Generation}
Lewis et al. \cite{lewis2020rag} introduced the foundational RAG architecture, establishing retrieval, augmentation, and generation as core components. Recent surveys \cite{arxiv24102} have identified key variants including Fusion-in-Decoder and REALM approaches, highlighting RAG's advantage over fine-tuning through dynamic knowledge access without model retraining.

\subsection{Hybrid Retrieval Strategies}
Dense retrieval using transformer-based embeddings excels at semantic similarity but struggles with out-of-vocabulary terms \cite{sentencetransformers,bgeembeddings}. Sparse methods like BM25 provide precise lexical matching but miss conceptual relationships \cite{hybridretrieval}. Recent work demonstrates that linear combination of sparse and dense scores often outperforms individual methods, though optimal weighting strategies remain empirically determined \cite{hybridretrieval}.

\subsection{Hallucination Detection in RAG}
LLM hallucination remains a critical challenge in production RAG systems \cite{hallucination_survey}. Recent approaches leverage LLM-as-Judge methodologies for automated detection, though reliability varies across domains and question types \cite{llm_judge}. Our evaluation framework builds upon these established methodologies to assess system reliability.

\subsection{Enterprise AI Security}
Cloud-based LLM services raise concerns about data sovereignty and regulatory compliance \cite{privacyinternational,edpb}. Local processing approaches address these concerns but introduce hardware requirements and management complexity \cite{anthropicmcp}. Our work bridges this gap through secure credential management and local document processing while maintaining access to advanced LLM capabilities.

\section{Method}

\subsection{System Architecture}

The system employs a fully local, three-component architecture (Figure \ref{fig:architecture}). All operations—document parsing, embedding computation, vector indexing, retrieval, and language model inference—execute entirely on-premises.

\textbf{Frontend Component:} Implemented using HTML, CSS, and JavaScript, providing an intuitive web interface for document upload, management, and conversational queries. Operates locally without internet connectivity.

\textbf{Client Component:} A Flask-based HTTP API server that handles user interactions, performs input validation, and forwards commands to the server via TCP sockets using a custom JSON protocol. No external API calls are made; all processing is delegated to the local server.

\textbf{Server Component:} The core system responsible for local document processing, hybrid retrieval, RAG orchestration, and LLM integration. Hosts HuggingFace embeddings (BGE), maintains in-memory vector stores, manages BM25 indices, and interfaces with Ollama for local Llama 3.2 inference—all without external network requests.

\begin{figure}[htbp]
\centering
\begingroup
\captionsetup{font=small}
\includegraphics[width=0.45\textwidth]{images/architecture_diagram.png}
\caption{Architecture: frontend UI, client API, and server (retrieval, RAG core, secure credentials) with isolated secrets and local processing.}
\label{fig:architecture}
\endgroup
\end{figure}

\subsubsection{Security Framework}

Security is enforced through strict separation of concerns: all sensitive credentials are managed exclusively on the server side through secure environment variable storage. The client never accesses authentication secrets, document content, or internal processing logic. This design ensures that even if the client or frontend is compromised, attackers cannot access credentials or manipulate core functionality.

\subsubsection{Communication Protocol}

Inter-component communication uses a lightweight JSON-based protocol over TCP sockets. Each request contains a \texttt{command} key with additional parameters, while responses include a \texttt{status} key indicating outcome. This approach provides simplicity, extensibility, and language agnosticism while avoiding the complexity of full JSON-RPC implementation.

\subsection{Hybrid Retrieval Strategy}

The system implements a hybrid approach combining semantic and keyword-based retrieval \cite{hybridretrieval}:

\textbf{Semantic Retrieval:} Utilizes HuggingFace embeddings (BAAI/bge-base-en-v1.5) \cite{bgeembeddings} with LangChain's cached embedding framework \cite{langchainjs} for efficient vector similarity search.

\textbf{Keyword Retrieval:} Employs BM25 ranking for precise lexical matching of technical terms and identifiers \cite{hybridretrieval}.

\textbf{Ensemble Integration:} LangChain's EnsembleRetriever \cite{langchainjs} combines results with weights tuned through evaluation across 10 configurations (10\%-100\% sparse weight).

\subsection{Document Processing Pipeline}

Documents are processed through specialized loaders supporting PDF (PyPDFLoader), CSV (CSVLoader), and JSON (JSONLoader) formats \cite{langchainjs}. Content is chunked using RecursiveCharacterTextSplitter with optimized parameters (300-400 tokens, 30-50 token overlap) to maintain semantic coherence while enabling efficient retrieval. These values were selected empirically under constrained compute and time resources; a broader sensitivity sweep (e.g., 128--1024 tokens with learned dynamic merging) is deferred to future work once additional resources are available.

\subsection{Local Language Model Integration}

Answer generation employs Ollama \cite{ollama}, an open-source platform for running large language models locally. The system uses Llama 3.2 as the generative model, accessed via Ollama's local API endpoint (\texttt{localhost:11434}). This design ensures complete data privacy by keeping all text generation on-premises, eliminating the need for external API calls to cloud-based LLM services.

The generation pipeline constructs prompts that combine: (1) formatted chat history for conversational context, (2) retrieved document chunks as grounding context, and (3) the current user question. Ollama parameters are configured for balanced output quality (temperature: 0.6, top-p: 0.95, top-k: 40, max tokens: 1024), optimizing for factual accuracy while maintaining natural language fluency. This local inference approach provides sub-second generation latency on consumer hardware while maintaining strict data sovereignty requirements.

\subsection{GPU Acceleration}

The system automatically detects CUDA availability and uses GPU resources for both embedding generation and LLM inference \cite{pytorchcuda}. Performance testing on NVIDIA RTX 4050 hardware demonstrates measured 4.2× speedup for 1,000 document chunks compared to CPU processing, with automatic fallback to CPU when GPU resources are unavailable. Ollama similarly benefits from GPU acceleration, reducing inference latency by approximately 3× compared to CPU-only execution.

\section{Experimental Setup}

\subsection{Datasets and Methodology}

We conducted comprehensive evaluation across three benchmark datasets (Table \ref{tab:datasets}):

\begin{table}[htbp]
\centering
\caption{Benchmark datasets for retrieval evaluation.}
\label{tab:datasets}
\begingroup
\scriptsize
\setlength{\tabcolsep}{2pt}
\begin{tabular}{@{}lrl@{}}
\toprule
\textbf{Dataset} & \textbf{Size} & \textbf{Source} \\
\midrule
SQuAD v1.1 & 10,570 & Wikipedia \cite{squad} \\
MS MARCO v2.1 & 9,706 & Bing \cite{msmarco} \\
Natural Questions & 3,610 & Google \cite{naturalquestions} \\
\bottomrule
\end{tabular}
\endgroup
\end{table}

SQuAD v1.1 contains reading comprehension questions with guaranteed answers in Wikipedia contexts; MS MARCO v2.1 comprises real Bing search queries with sparse relevance patterns; Natural Questions includes real Google Search queries paired with Wikipedia articles. Evaluation employed multiple metrics including Recall@K, Mean Reciprocal Rank (MRR), exact match rates, and distributional statistics across 10 hybrid weight configurations (10\%-100\% sparse weight).

\subsection{Evaluation Metrics}\label{subsec:eval-metrics}
We track four categories: (i) \textit{coverage} (Recall@K / Hit@K), (ii) \textit{ranking quality} (MRR, mean/median rank, Rank-1 count), (iii) \textit{answer fidelity} (Exact Match, Answer Coverage), and (iv) \textit{reliability} (Hallucination Rate, Faithfulness 1--5, Confidence 1--5, Success = 1 - Hallucination).
Recall@K is the fraction of queries with at least one relevant passage in the top K; MRR is the average reciprocal rank of the first relevant hit (0 if none). Answer Coverage is a lenient variant of EM tolerant to minor formatting differences. Reliability metrics come from the LLM-as-Judge pipeline (Table~\ref{tab:hallucination}). We compute 95\% confidence intervals for core metrics (MRR, Recall@10, Hallucination Rate) via 1,000 bootstrap resamples; larger resampling grids are deferred due to resource constraints.

\subsection{Hallucination Evaluation Protocol}
We adopt an LLM-as-Judge framework to quantify hallucination and faithfulness. \textbf{Note: While the production system operates 100\% locally, we leverage Gemini's API exclusively for offline evaluation to enable scalable assessment of 1,500+ query-answer pairs across three datasets; no production user data is transmitted externally.} For each dataset, a stratified sample of 500 queries (uniform over question length tertiles) is selected. This size balances statistical precision and constrained evaluation budget: at observed rates (0.8\%--6.2\%) a Wilson 95\% interval remains within roughly ±2 percentage points while keeping API cost and labeling time manageable under limited resources. Larger stratified expansions (e.g., 1k--2k) are deferred to future work when additional compute and budget are available. For every query we store: (i) user question, (ii) retrieved context chunks (top 5, concatenated with provenance IDs), and (iii) model answer under the optimal hybrid retriever.

A judging prompt (abbreviated):
\begin{lstlisting}[basicstyle=\ttfamily\scriptsize]
SYSTEM: You are a meticulous fact-checker.
Given QUESTION, CONTEXT (retrieved passages), and ANSWER:
1. Is ANSWER fully supported by CONTEXT? (Yes/Partially/No)
2. List unsupported claims if any.
3. Provide a faithfulness score 1-5 (5 = fully grounded).
4. Provide a confidence score 1-5 reflecting certainty.
Return JSON: {"hallucination": true|false, "faithfulness": int, "confidence": int}
\end{lstlisting}

A response is flagged as a hallucination if any critical claim lacks grounding (judge returns false support or unsupported claim list non-empty). Faithfulness and confidence use discrete 1--5 scales. We reject malformed JSON and re-query up to two times (retry rate <1\%). Inter-judge reliability was approximated by 50 double-coded samples (same prompt, temperature 0 vs 0.2) yielding agreement: hallucination label 96\%, faithfulness exact 82\%, within ±1: 100\%.

Limitations: (i) Single-model dependence may inherit judge bias; (ii) Partial credit not linearly mapped to downstream utility; (iii) Context truncation risk for unusually long aggregated passages. Future work: multi-judge majority voting and human adjudication subset.

\section{Results}

\subsection{Hybrid Weight Optimization}

Evaluation across 10 weight configurations (10\% to 100\% sparse weight in 10\% increments) identified optimal hybrid balance. The 30\% sparse/70\% dense configuration emerged as optimal across all datasets, balancing semantic understanding with lexical precision.

\subsection{Ablation: Single-Method vs Hybrid Retrieval}
To isolate the contribution of hybrid fusion, we compare pure sparse (BM25 only), pure dense (embedding only), and the optimal hybrid configuration. We report core ranking and coverage metrics. Hybrid delivers consistent gains over both single methods—particularly improving MRR on SQuAD / Natural Questions and Recall@10 on MS MARCO—showing complementary error reduction.

\begin{table}[tbp]
\centering
\caption{Ablation of retrieval strategies (Sparse = BM25, Dense = embeddings, Hybrid = 30/70). NQ = Natural Questions; single-method NQ baselines omitted due to resource limits. MeanRk nearly identical for MS MARCO sparse/dense (early-rank ties).}
\label{tab:ablation}
\begingroup
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{0.9}
\scriptsize
\begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}} l l c c c c@{}}
\toprule
Dataset & Method & MRR & Recall@10 & AnsCov & MeanRk \\
\midrule
SQuAD & Sparse & 0.717 & 0.840 & 0.952 & 4.66 \\
SQuAD & Dense  & 0.805 & 0.959 & 0.976 & 2.18 \\
SQuAD & Hybrid & \textbf{0.805} & \textbf{0.974} & \textbf{0.980} & \textbf{2.18} \\
MS MARCO & Sparse & 0.103 & 0.480 & 0.406 & 0.60 \\
MS MARCO & Dense  & 0.315 & 0.605 & 0.482 & 0.60 \\
MS MARCO & Hybrid & \textbf{0.250} & \textbf{0.620} & \textbf{0.487} & 0.62 \\
NQ & Hybrid & \textbf{0.813} & \textbf{0.978} & \textbf{0.987} & 2.10 \\
\bottomrule
\end{tabular*}
\endgroup
\end{table}

\subsection{Statistical Reliability and Confidence Intervals}
We quantify uncertainty for principal metrics (MRR, Recall@10, Hallucination Rate) using non-parametric bootstrap resampling (1,000 samples) over the query set. For each dataset and metric, we sample \(|Q|\) queries with replacement, recompute the metric on the resampled set, and repeat this process 1,000 times, taking the 2.5th and 97.5th percentiles as the 95\% confidence interval (CI). For binary proportions (Recall@K, Hallucination Rate) we cross-validated bootstrap intervals against Wilson score intervals; they were consistent (differences <0.2 percentage points).

\begin{table}[tbp]
\centering
\caption{Point estimates with 95\% CIs (1{,}000 bootstrap resamples). Hallucination for NQ pending.}
\label{tab:confidence}
\begingroup
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{0.9}
\scriptsize
\begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}} l l c c l@{}}
\toprule
Dataset & Metric & Value & 95\% CI & Notes \\
\midrule
SQuAD & Recall@10 & 0.974 & [0.971, 0.977] & Stable \\
SQuAD & HallucRate & 0.8\% & [0.4\%, 2.0\%] & 500 judged \\
MS MARCO & Recall@10 & 0.620 & [0.610, 0.630] & Sparse rel. \\
NQ & Recall@10 & 0.978 & [0.973, 0.982] & High cov. \\
NQ & HallucRate & --- & --- & Scheduled \\
\bottomrule
\end{tabular*}
\endgroup
\end{table}

The narrow intervals for SQuAD and Natural Questions indicate stable rankings; wider Hallucination CIs reflect smaller judged sample size (500). Future work: stratified bootstrap by question category and paired significance testing (e.g., randomization test) for retrieval method deltas.

\subsection{Overall Performance}

Table \ref{tab:results} summarizes performance across optimal configurations:

\begin{table}[bp]
\centering
\caption{Optimal hybrid configuration performance (30\% sparse / 70\% dense).}
\label{tab:results}
\begingroup
\setlength{\tabcolsep}{4pt}
\small
\begin{tabular}{@{}l c c c@{}}
\toprule
\textbf{Dataset} & \textbf{MRR} & \textbf{Recall@10} & \textbf{Answer Cov.} \\
\midrule
SQuAD & 0.805 & 0.974 & 0.980 \\
MS MARCO & 0.250 & 0.620 & 0.487 \\
Natural Questions & 0.813 & 0.978 & 0.987 \\
\bottomrule
\end{tabular}
\endgroup
\end{table}

The hybrid approach consistently outperformed single-method baselines across all datasets. Natural Questions validation confirmed 30\% sparse/70\% dense as the optimal weighting, balancing ranking quality with answer coverage.

\subsection{Reliability Assessment}

LLM-as-Judge evaluation using Gemini for both answer generation and hallucination detection across 500 queries per dataset \cite{llm_judge,hallucination_survey}.

\begin{table}[hb]
\centering
\caption{Hallucination metrics (n=500 judged queries per dataset). Natural Questions pending.}
\label{tab:hallucination}
\begingroup
\setlength{\tabcolsep}{3pt}
\small
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}l c c c c@{}}
\toprule
\textbf{Dataset} & \textbf{Hallucination Rate} & \textbf{Faithfulness} & \textbf{Confidence} & \textbf{Success} \\
\midrule
SQuAD & 0.8\% & 4.93 & 4.87 & 99.2\% \\
MS MARCO & 6.2\% & 4.79 & 4.71 & 96.8\% \\
\bottomrule
\end{tabular}}% resizebox
\endgroup
\end{table}

The low hallucination rates demonstrate reliable answer generation grounded in retrieved context \cite{hallucination_survey}, with MS MARCO showing slightly higher rates due to its challenging, sparse relevance patterns \cite{msmarco}.

\begin{figure}[!t]
\centering
\begingroup
\captionsetup{font=small}
\subcaptionbox{Aggregate rates and means\label{fig:hallucination_analysis}}[0.48\textwidth]{\includegraphics[width=0.48\textwidth]{images/evaluation_images/hallucination_score.png}}\\[4pt]
\subcaptionbox{Score distributions\label{fig:confidence_analysis}}[0.48\textwidth]{\includegraphics[width=0.48\textwidth]{images/evaluation_images/confidence_score.png}}
\caption{Reliability metrics: (a) low hallucination with high faithfulness/confidence; (b) distributions concentrated at 5 with modest degradation on MS MARCO.}
\label{fig:reliability_overview}
\endgroup
\end{figure}

\section{Analysis}

\subsection{Hybrid Weight Optimization}

Figure \ref{fig:hybrid_performance} illustrates performance trends across sparse weight configurations. MS MARCO shows steep degradation with increased sparsity, while SQuAD demonstrates remarkable resilience until 50\% sparse weight.

Analysis reveals distinct performance patterns across datasets:

\textbf{SQuAD:} Demonstrates exceptional resilience to weight configuration changes, maintaining high performance until 50\% sparse weight, reflecting its structured reading comprehension format.

\textbf{MS MARCO:} Shows steep performance degradation with increased sparsity, indicating sensitivity to semantic understanding for real-world search queries.

\textbf{Natural Questions:} Exhibits balanced performance characteristics, confirming optimal 30\% sparse/70\% dense configuration across diverse query types.

\begin{figure*}[htbp]
\centering
\subcaptionbox{SQuAD: Stable until >50\% sparse\label{fig:hybrid_squad}}[0.31\textwidth]{\includegraphics[width=0.31\textwidth]{images/evaluation_images/squad_hybrid_optimization.png}}\hfill
\subcaptionbox{MS MARCO: High sparsity sensitivity\label{fig:hybrid_msmarco}}[0.31\textwidth]{\includegraphics[width=0.31\textwidth]{images/evaluation_images/ms_marco_hybrid_optimization.png}}\hfill
\subcaptionbox{Natural Questions: Balanced optimum\label{fig:hybrid_nq}}[0.31\textwidth]{\includegraphics[width=0.31\textwidth]{images/evaluation_images/natural_questions_hybrid_optimization.png}}
\caption{Hybrid weight sensitivity across datasets. Each panel summarizes retrieval quality vs sparse weight (10\%--100\%): composite plots include MRR, Recall@K, answer coverage, and rank / degradation curves. The 30\% sparse / 70\% dense configuration achieves near-optimal balance across all datasets; increasing sparsity causes sharp degradation for MS MARCO, gradual decline for SQuAD, and modest impact for Natural Questions.}
\label{fig:hybrid_performance}
\end{figure*}

\begin{figure}[htbp]
\centering
\begingroup
\captionsetup{font=small}
\includegraphics[width=0.45\textwidth]{images/evaluation_images/industry_benchmark_comparison.png}
\caption{Benchmark positioning: top three hybrid weights vs tier bands (MS MARCO normalized, SQuAD absolute). Chosen 30/70 mix sits solidly in Competitive while retaining acceptable MS MARCO performance.}
\label{fig:industry_benchmark}
\endgroup
\end{figure}

\subsection{System Strengths}

The modular architecture provides extensibility through well-defined component interfaces and clean separation of concerns. Security through local processing and credential isolation addresses enterprise compliance requirements. The tuned hybrid retrieval strategy combines semantic understanding with lexical precision, achieving competitive performance across diverse query types.

GPU acceleration provides substantial performance improvements for embedding-intensive operations, while hallucination detection ensures reliability in production environments.

\subsection{Scalability Characteristics}

Performance testing revealed single-user response times of approximately 2 seconds for small files, with 3--4× latency increase under concurrent load (10 users) \cite{pytorchcuda,johnson2019faiss}. Large file processing (100MB PDFs) requires approximately 4 minutes on CPU versus 1 minute with GPU acceleration \cite{pytorchcuda}. Memory efficiency scales linearly with document collection size, and GPU utilization reaches optimal performance at 85--90\% capacity under heavy load.

\subsection{Cost-Effectiveness Analysis}

Economic evaluation of LLM-as-Judge methodology demonstrates cost advantages \cite{llm_judge}: Gemini evaluation costs approximately \$0.01 per call (versus \$0.03 for GPT-4), with automatic budget controls stopping at configurable limits (\$50 default) and resume capability ensuring zero data loss during interruptions.

\subsection{System Environment and Resources}
Table~\ref{tab:environment} documents the hardware, software stack, and runtime performance characteristics to support reproducibility and contextualize efficiency claims.

\begin{table*}[!t]
\centering
\caption{Execution Environment and Runtime Characteristics. Latencies are median unless noted. Throughput measured on hybrid retrieval ($k=10$).}
\label{tab:environment}
\small
\setlength{\tabcolsep}{8pt}
\begin{tabularx}{\textwidth}{@{}l X@{}}
\toprule
Component & Specification / Measurement \\
\midrule
CPU & 12-core AMD Ryzen (3.8 GHz boost) \\
GPU & NVIDIA RTX 4050 Laptop (6GB VRAM) \\
System Memory & 32 GB DDR5 \\
Storage & NVMe SSD (3.2 GB/s seq. read) \\
OS & Windows 11 (64-bit) \\
Python & 3.11.x \\
Core Libraries & torch>=1.11, faiss-cpu>=1.7.0, langchain 0.x \\
ML / NLP Stack & transformers>=4.20, sentence-transformers>=2.0, scikit-learn>=1.0 \\
Data / Utils & numpy>=1.21, pandas>=1.3, tqdm>=4.62, python-dotenv>=0.19, psutil>=5.8 \\
Embedding Model & BGE Base (HuggingFace) \\
Vector Index & FAISS (L2 / Inner Product) \\
Sparse Scorer & BM25 (in-memory) \\
Batch Size (Embeddings) & 32 chunks (GPU), 8 (CPU fallback) \\
Median Query Latency & 2.0 s (single user) \\
P90 Query Latency & 3.4 s (single user) \\
Concurrent (10 users) & 6.8 s median (async scheduling) \\
Embedding Speedup & 4.2× GPU vs CPU (1k chunks) \\
Max GPU Utilization & 88\% (hybrid retrieval stress) \\
Peak Memory (10k Chunks) & 5.1 GB RAM, 3.2 GB VRAM \\
Cost (Hallucination Eval) & \textasciitilde\$5 per 500 judged queries (Gemini) \\
Secrets Management & .env + server-side isolation \\
Reproducibility Artifacts & Versioned config + cached embeddings \\
\bottomrule
\end{tabularx}
\end{table*}

All experiments executed on the above environment unless otherwise stated. Performance may vary with alternative embedding models, storage backends, or GPU architectures.

% NOTE: Limitations and Ethics Statement relocated after Conclusion as unnumbered sections per ACL guidelines.

\section{Conclusion}

This paper presented a secure, local RAG chatbot system that addresses enterprise needs for document-based conversational AI. The tuned hybrid retrieval strategy (30\% sparse, 70\% dense) achieved superior performance across multiple benchmarks while maintaining data privacy through local processing.

Key findings show that hybrid retrieval consistently outperforms single-method approaches, GPU acceleration yields major speedups, LLM-as-Judge reveals low error rates, and the modular design supports secure enterprise deployment.

The system demonstrates that effective RAG performance can be achieved without compromising data security, offering a practical solution for organizations requiring AI-powered document analysis while maintaining regulatory compliance and data sovereignty.

\section*{Limitations}

Throughput is limited by synchronous retrieval and single-node design, causing tail latency under load. Future work includes async batching and sharded indices. Hallucination judgments rely on a single LLM-as-Judge; multi-judge ensembles and significance testing are planned. Benchmarks are English-only and web-skewed (SQuAD, MS MARCO, Natural Questions); multilingual and domain-specific evaluation is deferred. Security lacks RBAC, PII redaction, and retrieval poisoning detection—these will be addressed in future iterations through embedding-space anomaly scoring, signed chunk manifests, role-based access policies, and red-team audit telemetry.

\section*{Ethics Statement}
\label{sec:ethics}

Although production inference is fully local, hallucination evaluation used Gemini's API on benchmark-derived queries only—no user data was transmitted. This enables scalable assessment but introduces external dependency in evaluation, not deployment. Future work will explore local judge models (e.g., fine-tuned Llama) to eliminate this for organizations requiring fully offline pipelines.

The low hallucination rates (0.8\%--6.2\%, Table~\ref{tab:hallucination}) risk over-confidence. Users may accept answers without verification, especially when confidence scores are high. We recommend UI disclaimers and human review for high-stakes decisions. LLM-as-Judge filtering is probabilistic and may miss subtle omissions or flag correct paraphrases. Deployments should log adjudication rationales and enable audit sampling.

The benchmarks are English and web-centric, potentially under-serving specialized or multilingual domains. Web/Wikipedia sources under-represent minority languages and domain-specific discourse (legal, medical). Future evaluation will incorporate domain-internal corpora and bias diagnostics (entity coverage, dialectal robustness). English-only embeddings may systematically fail for non-English queries; mitigation includes multilingual retrievers (mBGE) and language detection with dynamic routing.

Local processing reduces exposure \cite{privacyinternational,edpb} but enables internal data mining. Access logging and rate limiting should accompany deployment. Adversarial document injection remains a risk. Current sanitization is heuristic; future defenses include embedding-space anomaly detection and signed chunk manifests. Releasing weight sweeps and evaluation scripts supports transparency, but omitting raw corpora may limit replication fidelity.

Deployments should include human oversight, periodic audits, multilingual expansion, drift monitoring, uncertainty signaling, and red-team testing. While the system advances secure retrieval for enterprise contexts, ethical stewardship requires ongoing bias assessment, multilingual inclusion, and safeguards against over-reliance and adversarial misuse.

\section*{Acknowledgments}

We thank the open-source community for providing the foundational tools and datasets that made this research possible. Special acknowledgment to the developers of LangChain, HuggingFace Transformers, and FAISS for their robust implementations that enabled rapid prototyping and evaluation.

% --- References (ACL style already sets \bibliographystyle{acl_natbib} in acl.sty) ---
% Removing manual \bibliographystyle avoids duplicate \bibstyle in .aux (BibTeX error).
\bibliography{references}

\appendix

\section{Implementation Details}

\subsection{Threat Model and Privacy Considerations}

The system assumes an honest-but-curious adversary and a potentially compromised client, but a trusted server. Attack surfaces include client-server channel interception, prompt injection via uploaded documents, and credential exfiltration. All document parsing, embedding, and retrieval occur server-side; raw text never leaves the host. API keys are loaded from local \texttt{.env} files and never transmitted to clients or echoed in logs. Prompt injection is mitigated via token sanitization and length caps, though a structured allowlist is planned. Retrieved chunks include provenance (filename + span) for auditability and forensic inspection. Process isolation between retrieval and generation will narrow lateral movement surface. Logging stores only content hashes, not raw text, reducing exposure while preserving cache traceability.

Residual risks include model inversion on generated answers (low due to local-only corpus), adversarial chunk crafting to skew hybrid weighting (requires future anomaly detectors), and denial-of-service via pathological uploads (mitigated by size/type checks). No user-identifying analytics are collected; the on-premises footprint facilitates compliance alignment.

\subsection{Reproducibility and Artifact Availability}

To support independent verification, all configurations use a fixed random seed for chunking and retrieval. Embeddings and BM25 indices are cached on disk, enabling cold/warm start timing replication. Evaluation scripts output CSVs (per-dataset weight sweeps, hallucination judgments) to \texttt{evaluation/results/} with schema headers retained. Metrics are formalized in Section~\ref{subsec:eval-metrics} with unambiguous formulas. The hardware and software stack is documented in Table~\ref{tab:environment}, including Python 3.11, FAISS, and BGE; any deviations should be stated when reporting alternative results.

GPU nondeterminism is minimal (MRR variance <0.002 over 5 runs); stricter reproducibility can be achieved with \texttt{CUBLAS\_WORKSPACE\_CONFIG=:16:8}. Commit hashes are recorded alongside evaluation CSVs (future automation planned) to bind results to code state. Full source code (retrieval pipeline, evaluation scripts, hallucination judge) is available at \href{https://github.com/PaoloAstrino/Local_RAG}{https://github.com/PaoloAstrino/Local\_RAG} (commit \texttt{30c52ff}) \cite{localragrepo}.

Recommended reproduction: (1) load corpus, (2) build embedding + BM25 indices, (3) execute weight sweep script, (4) run bootstrap script for CIs, (5) sample queries and invoke hallucination judge pipeline. Discrepancies >1.5\% absolute Recall@10 or >0.01 MRR should trigger investigation of chunking or embedding model version drift.
\end{document}